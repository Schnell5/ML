{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\", model=False, timestamp=False, subdirname=\"\"):\n",
    "    \"\"\"\n",
    "    - model - set True if model is saved (e.g. saver.save())\n",
    "    - timestamp - set True to create unique directory\n",
    "    \"\"\"\n",
    "    time = \"\"\n",
    "    root_dir = \"../models\" if model else \"../tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    if timestamp:\n",
    "        time = \"-\" + datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    name = prefix + \"model\" + time\n",
    "    logdir = \"{}/{}\".format(root_dir, name) if not subdirname else \"{}/{}/{}\".format(root_dir, subdirname, name)\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST image\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for a neuron layer - actually we don't need it since TF has its own implementation\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(log_dir(prefix=\"mnist_NN\"), tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is ready. Let's start a learning phase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n",
      "(60000,) uint8\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(X_train.shape, X_train.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0   # reshape and scale from 0.0 to 1.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_test = y_test.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, batch_size):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tBatch accuracy: 0.58273\tVal accuracy: 0.56600\n",
      "Epoch  1\tBatch accuracy: 0.71636\tVal accuracy: 0.73340\n",
      "Epoch  2\tBatch accuracy: 0.79818\tVal accuracy: 0.79740\n",
      "Epoch  3\tBatch accuracy: 0.82091\tVal accuracy: 0.82940\n",
      "Epoch  4\tBatch accuracy: 0.82727\tVal accuracy: 0.84940\n",
      "Epoch  5\tBatch accuracy: 0.83909\tVal accuracy: 0.86220\n",
      "Epoch  6\tBatch accuracy: 0.86182\tVal accuracy: 0.87000\n",
      "Epoch  7\tBatch accuracy: 0.84727\tVal accuracy: 0.87740\n",
      "Epoch  8\tBatch accuracy: 0.87909\tVal accuracy: 0.88400\n",
      "Epoch  9\tBatch accuracy: 0.87818\tVal accuracy: 0.88680\n",
      "Epoch 10\tBatch accuracy: 0.88727\tVal accuracy: 0.89040\n",
      "Epoch 11\tBatch accuracy: 0.88727\tVal accuracy: 0.89480\n",
      "Epoch 12\tBatch accuracy: 0.89273\tVal accuracy: 0.89780\n",
      "Epoch 13\tBatch accuracy: 0.90000\tVal accuracy: 0.90120\n",
      "Epoch 14\tBatch accuracy: 0.90182\tVal accuracy: 0.90440\n",
      "Epoch 15\tBatch accuracy: 0.90182\tVal accuracy: 0.90500\n",
      "Epoch 16\tBatch accuracy: 0.91000\tVal accuracy: 0.90900\n",
      "Epoch 17\tBatch accuracy: 0.89909\tVal accuracy: 0.90980\n",
      "Epoch 18\tBatch accuracy: 0.88818\tVal accuracy: 0.91220\n",
      "Epoch 19\tBatch accuracy: 0.89364\tVal accuracy: 0.91400\n",
      "Epoch 20\tBatch accuracy: 0.91545\tVal accuracy: 0.91520\n",
      "Epoch 21\tBatch accuracy: 0.90818\tVal accuracy: 0.91580\n",
      "Epoch 22\tBatch accuracy: 0.91091\tVal accuracy: 0.91800\n",
      "Epoch 23\tBatch accuracy: 0.90364\tVal accuracy: 0.91840\n",
      "Epoch 24\tBatch accuracy: 0.92364\tVal accuracy: 0.91940\n",
      "Epoch 25\tBatch accuracy: 0.91091\tVal accuracy: 0.91980\n",
      "Epoch 26\tBatch accuracy: 0.91818\tVal accuracy: 0.92140\n",
      "Epoch 27\tBatch accuracy: 0.92000\tVal accuracy: 0.92120\n",
      "Epoch 28\tBatch accuracy: 0.91727\tVal accuracy: 0.92380\n",
      "Epoch 29\tBatch accuracy: 0.92545\tVal accuracy: 0.92480\n",
      "Epoch 30\tBatch accuracy: 0.92545\tVal accuracy: 0.92580\n",
      "Epoch 31\tBatch accuracy: 0.91636\tVal accuracy: 0.92740\n",
      "Epoch 32\tBatch accuracy: 0.93273\tVal accuracy: 0.92800\n",
      "Epoch 33\tBatch accuracy: 0.92091\tVal accuracy: 0.92860\n",
      "Epoch 34\tBatch accuracy: 0.92636\tVal accuracy: 0.92940\n",
      "Epoch 35\tBatch accuracy: 0.91364\tVal accuracy: 0.92900\n",
      "Epoch 36\tBatch accuracy: 0.92000\tVal accuracy: 0.92960\n",
      "Epoch 37\tBatch accuracy: 0.91727\tVal accuracy: 0.93020\n",
      "Epoch 38\tBatch accuracy: 0.93000\tVal accuracy: 0.93060\n",
      "Epoch 39\tBatch accuracy: 0.92818\tVal accuracy: 0.93140\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tBatch accuracy: {:.5f}\\tVal accuracy: {:.5f}\".format(epoch, acc_train, acc_val))\n",
    "    save_path = saver.save(sess, log_dir(prefix=\"mnist_NN\", model=True, subdirname=\"mnist_NN_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/mnist_NN_model/mnist_NN-model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_path)\n",
    "    X_new = X_test[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Prediction  Correct\n",
       "0        7           7     True\n",
       "1        2           2     True\n",
       "2        1           1     True\n",
       "3        0           0     True\n",
       "4        4           4     True\n",
       "5        1           1     True\n",
       "6        4           4     True\n",
       "7        9           9     True\n",
       "8        5           6    False\n",
       "9        9           9     True\n",
       "10       0           0     True\n",
       "11       6           6     True\n",
       "12       9           9     True\n",
       "13       0           0     True\n",
       "14       1           1     True\n",
       "15       5           5     True\n",
       "16       9           9     True\n",
       "17       7           7     True\n",
       "18       3           3     True\n",
       "19       4           4     True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\"Actual\": y_test[:20], \"Prediction\": y_pred})\n",
    "results[\"Correct\"] = results[\"Actual\"] == results[\"Prediction\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ``tf.layer.dense`` in place of ``neuron_layer``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST image\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(prefix=\"TF_dense-mnistNN\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tBatch accuracy: 0.85818\tVal accuracy: 0.85580\n",
      "Epoch  1\tBatch accuracy: 0.90545\tVal accuracy: 0.89960\n",
      "Epoch  2\tBatch accuracy: 0.90091\tVal accuracy: 0.91140\n",
      "Epoch  3\tBatch accuracy: 0.92000\tVal accuracy: 0.92160\n",
      "Epoch  4\tBatch accuracy: 0.92000\tVal accuracy: 0.92960\n",
      "Epoch  5\tBatch accuracy: 0.92182\tVal accuracy: 0.92520\n",
      "Epoch  6\tBatch accuracy: 0.94091\tVal accuracy: 0.93860\n",
      "Epoch  7\tBatch accuracy: 0.95182\tVal accuracy: 0.93940\n",
      "Epoch  8\tBatch accuracy: 0.95545\tVal accuracy: 0.94300\n",
      "Epoch  9\tBatch accuracy: 0.95273\tVal accuracy: 0.94620\n",
      "Epoch 10\tBatch accuracy: 0.94818\tVal accuracy: 0.94880\n",
      "Epoch 11\tBatch accuracy: 0.95545\tVal accuracy: 0.95000\n",
      "Epoch 12\tBatch accuracy: 0.94182\tVal accuracy: 0.95100\n",
      "Epoch 13\tBatch accuracy: 0.95091\tVal accuracy: 0.95440\n",
      "Epoch 14\tBatch accuracy: 0.95636\tVal accuracy: 0.95540\n",
      "Epoch 15\tBatch accuracy: 0.95455\tVal accuracy: 0.95680\n",
      "Epoch 16\tBatch accuracy: 0.95364\tVal accuracy: 0.95820\n",
      "Epoch 17\tBatch accuracy: 0.96000\tVal accuracy: 0.95980\n",
      "Epoch 18\tBatch accuracy: 0.95364\tVal accuracy: 0.96240\n",
      "Epoch 19\tBatch accuracy: 0.94818\tVal accuracy: 0.96200\n",
      "Epoch 20\tBatch accuracy: 0.95818\tVal accuracy: 0.96280\n",
      "Epoch 21\tBatch accuracy: 0.96364\tVal accuracy: 0.96420\n",
      "Epoch 22\tBatch accuracy: 0.96000\tVal accuracy: 0.96420\n",
      "Epoch 23\tBatch accuracy: 0.97636\tVal accuracy: 0.96620\n",
      "Epoch 24\tBatch accuracy: 0.96455\tVal accuracy: 0.96800\n",
      "Epoch 25\tBatch accuracy: 0.97636\tVal accuracy: 0.96740\n",
      "Epoch 26\tBatch accuracy: 0.97909\tVal accuracy: 0.96960\n",
      "Epoch 27\tBatch accuracy: 0.97636\tVal accuracy: 0.96900\n",
      "Epoch 28\tBatch accuracy: 0.96636\tVal accuracy: 0.96940\n",
      "Epoch 29\tBatch accuracy: 0.97727\tVal accuracy: 0.96940\n",
      "Epoch 30\tBatch accuracy: 0.97364\tVal accuracy: 0.97040\n",
      "Epoch 31\tBatch accuracy: 0.97273\tVal accuracy: 0.97080\n",
      "Epoch 32\tBatch accuracy: 0.96818\tVal accuracy: 0.97060\n",
      "Epoch 33\tBatch accuracy: 0.96818\tVal accuracy: 0.97160\n",
      "Epoch 34\tBatch accuracy: 0.98182\tVal accuracy: 0.97300\n",
      "Epoch 35\tBatch accuracy: 0.97636\tVal accuracy: 0.97180\n",
      "Epoch 36\tBatch accuracy: 0.97636\tVal accuracy: 0.97280\n",
      "Epoch 37\tBatch accuracy: 0.98455\tVal accuracy: 0.97360\n",
      "Epoch 38\tBatch accuracy: 0.97636\tVal accuracy: 0.97300\n",
      "Epoch 39\tBatch accuracy: 0.98091\tVal accuracy: 0.97320\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tBatch accuracy: {:.5f}\\tVal accuracy: {:.5f}\".format(epoch, accuracy_batch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"TF_dense-mnistNN\", model=True, subdirname=\"TF_dense_mnistNN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mu_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}\\mathbf{x}^{(i)}$\n",
    "2. $\\sigma_B^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}(\\mathbf{x}^{(i)}-\\mu_B)^2$\n",
    "3. $\\hat{\\mathbf{x}}^{(i)}=\\dfrac{\\mathbf{x}^{(i)}-\\mu_B}{\\sqrt{\\mu_B^2+\\varepsilon}}$\n",
    "4. $\\mathbf{z}^{(i)} = \\gamma\\hat{\\mathbf{x}}^{(i)} + \\beta$\n",
    "\n",
    "\n",
    "- $\\mu_B$ - empirical average value (counted on mini-batch B)\n",
    "- $\\sigma_B$ - empirical standart deviation (counted on mini-batch B)\n",
    "- $m_B$ - number of samples in mini-batch B\n",
    "- $\\hat{\\mathbf{x}}^{(i)}$ - centered around 0 and normalized input\n",
    "- $\\gamma$ - scalability parameter for a layer\n",
    "- $\\beta$ - shift (bias) value for a layer\n",
    "- $\\varepsilon$ - smoothing term (usually $10^{-5}$)\n",
    "- $\\mathbf{z}^{(i)}$ - BN operation output (scaled and shifted version of inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "outputs = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, outputs, name=\"logits\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"BatchNormalized_model\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update $\\gamma$ and $\\beta$ we have to evaluate some extra operations to calculate them (in addition to training_op node evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tAccuracy: 0.93520\n",
      "Epoch  1\tAccuracy: 0.95460\n",
      "Epoch  2\tAccuracy: 0.96260\n",
      "Epoch  3\tAccuracy: 0.96900\n",
      "Epoch  4\tAccuracy: 0.97100\n",
      "Epoch  5\tAccuracy: 0.97100\n",
      "Epoch  6\tAccuracy: 0.97380\n",
      "Epoch  7\tAccuracy: 0.97320\n",
      "Epoch  8\tAccuracy: 0.97420\n",
      "Epoch  9\tAccuracy: 0.97200\n",
      "Epoch 10\tAccuracy: 0.97620\n",
      "Epoch 11\tAccuracy: 0.97640\n",
      "Epoch 12\tAccuracy: 0.97760\n",
      "Epoch 13\tAccuracy: 0.97860\n",
      "Epoch 14\tAccuracy: 0.97760\n",
      "Epoch 15\tAccuracy: 0.97800\n",
      "Epoch 16\tAccuracy: 0.97780\n",
      "Epoch 17\tAccuracy: 0.97940\n",
      "Epoch 18\tAccuracy: 0.97800\n",
      "Epoch 19\tAccuracy: 0.98060\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tAccuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    saver.save(sess, log_dir(\"BatchNorm\", model=True, subdirname=\"BatchNorm_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    threshold = 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"GradientClipping_model\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.91540\n",
      "Epoch  1\tValid accuracy: 0.94820\n",
      "Epoch  2\tValid accuracy: 0.95920\n",
      "Epoch  3\tValid accuracy: 0.95180\n",
      "Epoch  4\tValid accuracy: 0.96900\n",
      "Epoch  5\tValid accuracy: 0.95860\n",
      "Epoch  6\tValid accuracy: 0.97240\n",
      "Epoch  7\tValid accuracy: 0.97440\n",
      "Epoch  8\tValid accuracy: 0.97500\n",
      "Epoch  9\tValid accuracy: 0.97680\n",
      "Epoch 10\tValid accuracy: 0.97640\n",
      "Epoch 11\tValid accuracy: 0.97600\n",
      "Epoch 12\tValid accuracy: 0.96900\n",
      "Epoch 13\tValid accuracy: 0.97720\n",
      "Epoch 14\tValid accuracy: 0.97940\n",
      "Epoch 15\tValid accuracy: 0.97360\n",
      "Epoch 16\tValid accuracy: 0.97560\n",
      "Epoch 17\tValid accuracy: 0.97980\n",
      "Epoch 18\tValid accuracy: 0.98000\n",
      "Epoch 19\tValid accuracy: 0.97980\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    saver.save(sess, log_dir(\"GradientClipping\", model=True, subdirname=\"GradClipp_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\", model=False, timestamp=False, subdirname=\"\"):\n",
    "    \"\"\"\n",
    "    - model - set True if model is saved (e.g. saver.save())\n",
    "    - timestamp - set True to create unique directory\n",
    "    \"\"\"\n",
    "    time = \"\"\n",
    "    root_dir = \"../models\" if model else \"../tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    if timestamp:\n",
    "        time = \"-\" + datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    name = prefix + \"model\" + time\n",
    "    logdir = \"{}/{}\".format(root_dir, name) if not subdirname else \"{}/{}/{}\".format(root_dir, subdirname, name)\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST image\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for a neuron layer - actually we don't need it since TF has its own implementation\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(log_dir(prefix=\"mnist_NN\"), tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is ready. Let's start a learning phase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n",
      "(60000,) uint8\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(X_train.shape, X_train.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0   # reshape and scale from 0.0 to 1.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_test = y_test.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1]), array([2, 3]), array([4, 5]), array([6, 7]), array([8, 9])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "np.array_split(a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tBatch accuracy: 0.90000\tVal accuracy: 0.91460\n",
      "Epoch  1\tBatch accuracy: 0.94000\tVal accuracy: 0.93240\n",
      "Epoch  2\tBatch accuracy: 0.94000\tVal accuracy: 0.94260\n",
      "Epoch  3\tBatch accuracy: 0.90000\tVal accuracy: 0.94900\n",
      "Epoch  4\tBatch accuracy: 0.96000\tVal accuracy: 0.95500\n",
      "Epoch  5\tBatch accuracy: 0.94000\tVal accuracy: 0.95560\n",
      "Epoch  6\tBatch accuracy: 1.00000\tVal accuracy: 0.96080\n",
      "Epoch  7\tBatch accuracy: 0.94000\tVal accuracy: 0.96180\n",
      "Epoch  8\tBatch accuracy: 0.98000\tVal accuracy: 0.96340\n",
      "Epoch  9\tBatch accuracy: 0.96000\tVal accuracy: 0.96380\n",
      "Epoch 10\tBatch accuracy: 0.92000\tVal accuracy: 0.96640\n",
      "Epoch 11\tBatch accuracy: 0.98000\tVal accuracy: 0.96740\n",
      "Epoch 12\tBatch accuracy: 0.98000\tVal accuracy: 0.96720\n",
      "Epoch 13\tBatch accuracy: 0.98000\tVal accuracy: 0.96960\n",
      "Epoch 14\tBatch accuracy: 1.00000\tVal accuracy: 0.97140\n",
      "Epoch 15\tBatch accuracy: 0.96000\tVal accuracy: 0.97240\n",
      "Epoch 16\tBatch accuracy: 1.00000\tVal accuracy: 0.97380\n",
      "Epoch 17\tBatch accuracy: 1.00000\tVal accuracy: 0.97380\n",
      "Epoch 18\tBatch accuracy: 1.00000\tVal accuracy: 0.97500\n",
      "Epoch 19\tBatch accuracy: 0.98000\tVal accuracy: 0.97480\n",
      "Epoch 20\tBatch accuracy: 1.00000\tVal accuracy: 0.97500\n",
      "Epoch 21\tBatch accuracy: 1.00000\tVal accuracy: 0.97580\n",
      "Epoch 22\tBatch accuracy: 0.98000\tVal accuracy: 0.97580\n",
      "Epoch 23\tBatch accuracy: 0.98000\tVal accuracy: 0.97560\n",
      "Epoch 24\tBatch accuracy: 0.98000\tVal accuracy: 0.97560\n",
      "Epoch 25\tBatch accuracy: 1.00000\tVal accuracy: 0.97680\n",
      "Epoch 26\tBatch accuracy: 0.98000\tVal accuracy: 0.97680\n",
      "Epoch 27\tBatch accuracy: 1.00000\tVal accuracy: 0.97760\n",
      "Epoch 28\tBatch accuracy: 0.94000\tVal accuracy: 0.97620\n",
      "Epoch 29\tBatch accuracy: 1.00000\tVal accuracy: 0.97680\n",
      "Epoch 30\tBatch accuracy: 1.00000\tVal accuracy: 0.97720\n",
      "Epoch 31\tBatch accuracy: 0.96000\tVal accuracy: 0.97780\n",
      "Epoch 32\tBatch accuracy: 0.96000\tVal accuracy: 0.97720\n",
      "Epoch 33\tBatch accuracy: 0.98000\tVal accuracy: 0.97900\n",
      "Epoch 34\tBatch accuracy: 1.00000\tVal accuracy: 0.97760\n",
      "Epoch 35\tBatch accuracy: 1.00000\tVal accuracy: 0.97700\n",
      "Epoch 36\tBatch accuracy: 0.98000\tVal accuracy: 0.97820\n",
      "Epoch 37\tBatch accuracy: 1.00000\tVal accuracy: 0.97720\n",
      "Epoch 38\tBatch accuracy: 1.00000\tVal accuracy: 0.97920\n",
      "Epoch 39\tBatch accuracy: 1.00000\tVal accuracy: 0.97880\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tBatch accuracy: {:.5f}\\tVal accuracy: {:.5f}\".format(epoch, acc_train, acc_val))\n",
    "    save_path = saver.save(sess, log_dir(prefix=\"mnist_NN\", model=True, subdirname=\"mnist_NN_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/mnist_NN_model/mnist_NN-model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_path)\n",
    "    X_new = X_test[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Prediction  Correct\n",
       "0        7           7     True\n",
       "1        2           2     True\n",
       "2        1           1     True\n",
       "3        0           0     True\n",
       "4        4           4     True\n",
       "5        1           1     True\n",
       "6        4           4     True\n",
       "7        9           9     True\n",
       "8        5           5     True\n",
       "9        9           9     True\n",
       "10       0           0     True\n",
       "11       6           6     True\n",
       "12       9           9     True\n",
       "13       0           0     True\n",
       "14       1           1     True\n",
       "15       5           5     True\n",
       "16       9           9     True\n",
       "17       7           7     True\n",
       "18       3           3     True\n",
       "19       4           4     True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\"Actual\": y_test[:20], \"Prediction\": y_pred})\n",
    "results[\"Correct\"] = results[\"Actual\"] == results[\"Prediction\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ``tf.layer.dense`` in place of ``neuron_layer``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST image\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(prefix=\"TF_dense-mnistNN\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tBatch accuracy: 0.90000\tVal accuracy: 0.90240\n",
      "Epoch  1\tBatch accuracy: 0.92000\tVal accuracy: 0.92540\n",
      "Epoch  2\tBatch accuracy: 0.94000\tVal accuracy: 0.93720\n",
      "Epoch  3\tBatch accuracy: 0.90000\tVal accuracy: 0.94160\n",
      "Epoch  4\tBatch accuracy: 0.94000\tVal accuracy: 0.94700\n",
      "Epoch  5\tBatch accuracy: 0.94000\tVal accuracy: 0.95140\n",
      "Epoch  6\tBatch accuracy: 1.00000\tVal accuracy: 0.95480\n",
      "Epoch  7\tBatch accuracy: 0.94000\tVal accuracy: 0.96100\n",
      "Epoch  8\tBatch accuracy: 0.96000\tVal accuracy: 0.96220\n",
      "Epoch  9\tBatch accuracy: 0.94000\tVal accuracy: 0.96500\n",
      "Epoch 10\tBatch accuracy: 0.92000\tVal accuracy: 0.96540\n",
      "Epoch 11\tBatch accuracy: 0.98000\tVal accuracy: 0.96660\n",
      "Epoch 12\tBatch accuracy: 0.98000\tVal accuracy: 0.96820\n",
      "Epoch 13\tBatch accuracy: 0.98000\tVal accuracy: 0.97040\n",
      "Epoch 14\tBatch accuracy: 1.00000\tVal accuracy: 0.96960\n",
      "Epoch 15\tBatch accuracy: 0.94000\tVal accuracy: 0.97180\n",
      "Epoch 16\tBatch accuracy: 0.98000\tVal accuracy: 0.97280\n",
      "Epoch 17\tBatch accuracy: 1.00000\tVal accuracy: 0.97320\n",
      "Epoch 18\tBatch accuracy: 0.98000\tVal accuracy: 0.97480\n",
      "Epoch 19\tBatch accuracy: 0.98000\tVal accuracy: 0.97560\n",
      "Epoch 20\tBatch accuracy: 1.00000\tVal accuracy: 0.97500\n",
      "Epoch 21\tBatch accuracy: 1.00000\tVal accuracy: 0.97340\n",
      "Epoch 22\tBatch accuracy: 0.96000\tVal accuracy: 0.97500\n",
      "Epoch 23\tBatch accuracy: 0.98000\tVal accuracy: 0.97640\n",
      "Epoch 24\tBatch accuracy: 0.98000\tVal accuracy: 0.97580\n",
      "Epoch 25\tBatch accuracy: 1.00000\tVal accuracy: 0.97600\n",
      "Epoch 26\tBatch accuracy: 0.92000\tVal accuracy: 0.97680\n",
      "Epoch 27\tBatch accuracy: 1.00000\tVal accuracy: 0.97740\n",
      "Epoch 28\tBatch accuracy: 0.94000\tVal accuracy: 0.97800\n",
      "Epoch 29\tBatch accuracy: 0.98000\tVal accuracy: 0.97800\n",
      "Epoch 30\tBatch accuracy: 1.00000\tVal accuracy: 0.97740\n",
      "Epoch 31\tBatch accuracy: 1.00000\tVal accuracy: 0.97780\n",
      "Epoch 32\tBatch accuracy: 0.96000\tVal accuracy: 0.97780\n",
      "Epoch 33\tBatch accuracy: 0.98000\tVal accuracy: 0.97840\n",
      "Epoch 34\tBatch accuracy: 0.98000\tVal accuracy: 0.97820\n",
      "Epoch 35\tBatch accuracy: 1.00000\tVal accuracy: 0.97800\n",
      "Epoch 36\tBatch accuracy: 1.00000\tVal accuracy: 0.97940\n",
      "Epoch 37\tBatch accuracy: 1.00000\tVal accuracy: 0.97860\n",
      "Epoch 38\tBatch accuracy: 0.98000\tVal accuracy: 0.98000\n",
      "Epoch 39\tBatch accuracy: 1.00000\tVal accuracy: 0.97860\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tBatch accuracy: {:.5f}\\tVal accuracy: {:.5f}\".format(epoch, accuracy_batch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"TF_dense-mnistNN\", model=True, subdirname=\"TF_dense_mnistNN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mu_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}\\mathbf{x}^{(i)}$\n",
    "2. $\\sigma_B^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}(\\mathbf{x}^{(i)}-\\mu_B)^2$\n",
    "3. $\\hat{\\mathbf{x}}^{(i)}=\\dfrac{\\mathbf{x}^{(i)}-\\mu_B}{\\sqrt{\\sigma_B^2+\\varepsilon}}$\n",
    "4. $\\mathbf{z}^{(i)} = \\gamma\\hat{\\mathbf{x}}^{(i)} + \\beta$\n",
    "\n",
    "\n",
    "- $\\mu_B$ - empirical average value (counted on mini-batch B)\n",
    "- $\\sigma_B$ - empirical standart deviation (counted on mini-batch B)\n",
    "- $m_B$ - number of samples in mini-batch B\n",
    "- $\\hat{\\mathbf{x}}^{(i)}$ - centered around 0 and normalized input\n",
    "- $\\gamma$ - scalability parameter for a layer\n",
    "- $\\beta$ - shift (bias) value for a layer\n",
    "- $\\varepsilon$ - smoothing term (usually $10^{-5}$)\n",
    "- $\\mathbf{z}^{(i)}$ - BN operation output (scaled and shifted version of inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "outputs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, outputs, name=\"logits\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"BatchNormalized_model\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update $\\gamma$ and $\\beta$ we have to evaluate some extra operations to calculate them (in addition to training_op node evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tAccuracy: 0.88240\n",
      "Epoch  1\tAccuracy: 0.90940\n",
      "Epoch  2\tAccuracy: 0.91960\n",
      "Epoch  3\tAccuracy: 0.92820\n",
      "Epoch  4\tAccuracy: 0.93180\n",
      "Epoch  5\tAccuracy: 0.93760\n",
      "Epoch  6\tAccuracy: 0.94380\n",
      "Epoch  7\tAccuracy: 0.94700\n",
      "Epoch  8\tAccuracy: 0.94920\n",
      "Epoch  9\tAccuracy: 0.95200\n",
      "Epoch 10\tAccuracy: 0.95360\n",
      "Epoch 11\tAccuracy: 0.95520\n",
      "Epoch 12\tAccuracy: 0.95760\n",
      "Epoch 13\tAccuracy: 0.96220\n",
      "Epoch 14\tAccuracy: 0.96320\n",
      "Epoch 15\tAccuracy: 0.96280\n",
      "Epoch 16\tAccuracy: 0.96580\n",
      "Epoch 17\tAccuracy: 0.96760\n",
      "Epoch 18\tAccuracy: 0.96780\n",
      "Epoch 19\tAccuracy: 0.96860\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tAccuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"BatchNorm\", model=True, subdirname=\"BatchNorm_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    threshold = 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"GradientClipping_model\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.28760\n",
      "Epoch  1\tValid accuracy: 0.79400\n",
      "Epoch  2\tValid accuracy: 0.87980\n",
      "Epoch  3\tValid accuracy: 0.90620\n",
      "Epoch  4\tValid accuracy: 0.91640\n",
      "Epoch  5\tValid accuracy: 0.92240\n",
      "Epoch  6\tValid accuracy: 0.92940\n",
      "Epoch  7\tValid accuracy: 0.93560\n",
      "Epoch  8\tValid accuracy: 0.93840\n",
      "Epoch  9\tValid accuracy: 0.94180\n",
      "Epoch 10\tValid accuracy: 0.94580\n",
      "Epoch 11\tValid accuracy: 0.94720\n",
      "Epoch 12\tValid accuracy: 0.94760\n",
      "Epoch 13\tValid accuracy: 0.95340\n",
      "Epoch 14\tValid accuracy: 0.95660\n",
      "Epoch 15\tValid accuracy: 0.95640\n",
      "Epoch 16\tValid accuracy: 0.95760\n",
      "Epoch 17\tValid accuracy: 0.95920\n",
      "Epoch 18\tValid accuracy: 0.96260\n",
      "Epoch 19\tValid accuracy: 0.96140\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping\", model=True, subdirname=\"GradClipp_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing TensorFlow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"../models/GradClipp_model/GradientClipping-model.meta\")  # Upload whole graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "train/gradients/Shape\n",
      "train/gradients/grad_ys_0\n",
      "train/gradients/Fill\n",
      "train/gradients/loss/loss_grad/Reshape/shape\n",
      "train/gradients/loss/loss_grad/Reshape\n",
      "train/gradients/loss/loss_grad/Shape\n",
      "train/gradients/loss/loss_grad/Tile\n",
      "train/gradients/loss/loss_grad/Shape_1\n",
      "train/gradients/loss/loss_grad/Shape_2\n",
      "train/gradients/loss/loss_grad/Const\n",
      "train/gradients/loss/loss_grad/Prod\n",
      "train/gradients/loss/loss_grad/Const_1\n",
      "train/gradients/loss/loss_grad/Prod_1\n",
      "train/gradients/loss/loss_grad/Maximum/y\n",
      "train/gradients/loss/loss_grad/Maximum\n",
      "train/gradients/loss/loss_grad/floordiv\n",
      "train/gradients/loss/loss_grad/Cast\n",
      "train/gradients/loss/loss_grad/truediv\n",
      "train/gradients/zeros_like\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "train/clip_by_value/Minimum/y\n",
      "train/clip_by_value/Minimum\n",
      "train/clip_by_value/y\n",
      "train/clip_by_value\n",
      "train/clip_by_value_1/Minimum/y\n",
      "train/clip_by_value_1/Minimum\n",
      "train/clip_by_value_1/y\n",
      "train/clip_by_value_1\n",
      "train/clip_by_value_2/Minimum/y\n",
      "train/clip_by_value_2/Minimum\n",
      "train/clip_by_value_2/y\n",
      "train/clip_by_value_2\n",
      "train/clip_by_value_3/Minimum/y\n",
      "train/clip_by_value_3/Minimum\n",
      "train/clip_by_value_3/y\n",
      "train/clip_by_value_3\n",
      "train/clip_by_value_4/Minimum/y\n",
      "train/clip_by_value_4/Minimum\n",
      "train/clip_by_value_4/y\n",
      "train/clip_by_value_4\n",
      "train/clip_by_value_5/Minimum/y\n",
      "train/clip_by_value_5/Minimum\n",
      "train/clip_by_value_5/y\n",
      "train/clip_by_value_5\n",
      "train/clip_by_value_6/Minimum/y\n",
      "train/clip_by_value_6/Minimum\n",
      "train/clip_by_value_6/y\n",
      "train/clip_by_value_6\n",
      "train/clip_by_value_7/Minimum/y\n",
      "train/clip_by_value_7/Minimum\n",
      "train/clip_by_value_7/y\n",
      "train/clip_by_value_7\n",
      "train/clip_by_value_8/Minimum/y\n",
      "train/clip_by_value_8/Minimum\n",
      "train/clip_by_value_8/y\n",
      "train/clip_by_value_8\n",
      "train/clip_by_value_9/Minimum/y\n",
      "train/clip_by_value_9/Minimum\n",
      "train/clip_by_value_9/y\n",
      "train/clip_by_value_9\n",
      "train/clip_by_value_10/Minimum/y\n",
      "train/clip_by_value_10/Minimum\n",
      "train/clip_by_value_10/y\n",
      "train/clip_by_value_10\n",
      "train/clip_by_value_11/Minimum/y\n",
      "train/clip_by_value_11/Minimum\n",
      "train/clip_by_value_11/y\n",
      "train/clip_by_value_11\n",
      "train/GradientDescent/learning_rate\n",
      "train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "train/GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to create descriptors for the train phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start the train phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/GradClipp_model/GradientClipping-model\n",
      "Epoch  0\tValid accuracy: 0.98060\n",
      "Epoch  1\tValid accuracy: 0.98100\n",
      "Epoch  2\tValid accuracy: 0.97940\n",
      "Epoch  3\tValid accuracy: 0.98160\n",
      "Epoch  4\tValid accuracy: 0.98040\n",
      "Epoch  5\tValid accuracy: 0.98080\n",
      "Epoch  6\tValid accuracy: 0.98200\n",
      "Epoch  7\tValid accuracy: 0.98120\n",
      "Epoch  8\tValid accuracy: 0.98100\n",
      "Epoch  9\tValid accuracy: 0.98120\n",
      "Epoch 10\tValid accuracy: 0.98040\n",
      "Epoch 11\tValid accuracy: 0.98140\n",
      "Epoch 12\tValid accuracy: 0.98040\n",
      "Epoch 13\tValid accuracy: 0.98100\n",
      "Epoch 14\tValid accuracy: 0.98040\n",
      "Epoch 15\tValid accuracy: 0.98040\n",
      "Epoch 16\tValid accuracy: 0.97960\n",
      "Epoch 17\tValid accuracy: 0.98020\n",
      "Epoch 18\tValid accuracy: 0.98120\n",
      "Epoch 19\tValid accuracy: 0.98060\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"../models/GradClipp_model/GradientClipping-model\")  # Restore the model\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping_NEW\", model=True, subdirname=\"GradClipp_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can use only specific parts of the uploaded graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20 # new\n",
    "n_outputs = 10 # new\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")  # new\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")  # new\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a new graph and we want to use hidden1, hidden2 and hidden3 layers from the uploaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/GradClipp_model/GradientClipping-model\n",
      "Epoch  0\tValid accuracy: 0.97200\n",
      "Epoch  1\tValid accuracy: 0.97820\n",
      "Epoch  2\tValid accuracy: 0.97780\n",
      "Epoch  3\tValid accuracy: 0.97940\n",
      "Epoch  4\tValid accuracy: 0.97940\n",
      "Epoch  5\tValid accuracy: 0.98000\n",
      "Epoch  6\tValid accuracy: 0.97940\n",
      "Epoch  7\tValid accuracy: 0.98000\n",
      "Epoch  8\tValid accuracy: 0.98140\n",
      "Epoch  9\tValid accuracy: 0.98080\n",
      "Epoch 10\tValid accuracy: 0.97980\n",
      "Epoch 11\tValid accuracy: 0.98080\n",
      "Epoch 12\tValid accuracy: 0.97780\n",
      "Epoch 13\tValid accuracy: 0.98040\n",
      "Epoch 14\tValid accuracy: 0.98020\n",
      "Epoch 15\tValid accuracy: 0.98000\n",
      "Epoch 16\tValid accuracy: 0.97940\n",
      "Epoch 17\tValid accuracy: 0.97980\n",
      "Epoch 18\tValid accuracy: 0.97980\n",
      "Epoch 19\tValid accuracy: 0.98000\n",
      "Total training time: 28.16\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")  #RegExp\n",
    "restore_saver = tf.train.Saver(reuse_vars)  # to restore layers 1-3 only\n",
    "\n",
    "t0 = time.time()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"../models/GradClipp_model/GradientClipping-model\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping_NEW\", model=True, subdirname=\"GradClipp_model\"))\n",
    "t1 = time.time()\n",
    "print(\"Total training time: {:.2f}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20 # new\n",
    "n_outputs = 10 # new\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")  # new\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")  # new\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")  # layers to train\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)  # train only specific layers\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")  #RegExp\n",
    "restore_saver = tf.train.Saver(reuse_vars)  # to restore layers 1-3 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/GradClipp_model/GradientClipping-model\n",
      "Epoch  0\tValid accuracy: 0.97340\n",
      "Epoch  1\tValid accuracy: 0.97620\n",
      "Epoch  2\tValid accuracy: 0.97720\n",
      "Epoch  3\tValid accuracy: 0.97880\n",
      "Epoch  4\tValid accuracy: 0.97880\n",
      "Epoch  5\tValid accuracy: 0.97980\n",
      "Epoch  6\tValid accuracy: 0.97940\n",
      "Epoch  7\tValid accuracy: 0.97900\n",
      "Epoch  8\tValid accuracy: 0.97940\n",
      "Epoch  9\tValid accuracy: 0.97940\n",
      "Epoch 10\tValid accuracy: 0.97940\n",
      "Epoch 11\tValid accuracy: 0.97940\n",
      "Epoch 12\tValid accuracy: 0.97940\n",
      "Epoch 13\tValid accuracy: 0.97900\n",
      "Epoch 14\tValid accuracy: 0.98020\n",
      "Epoch 15\tValid accuracy: 0.98000\n",
      "Epoch 16\tValid accuracy: 0.97920\n",
      "Epoch 17\tValid accuracy: 0.97960\n",
      "Epoch 18\tValid accuracy: 0.97940\n",
      "Epoch 19\tValid accuracy: 0.98020\n",
      "Total training time: 14.63\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"../models/GradClipp_model/GradientClipping-model\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping_NEW\", model=True, subdirname=\"GradClipp_model\"))\n",
    "t1 = time.time()\n",
    "print(\"Total training time: {:.2f}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with two freezed layers the training process goes ~2 times faster than in previous case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can use ``stop_gradient`` method to freeze all layers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20 # new\n",
    "n_outputs = 10 # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")  # freezed\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")  # freezed\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")  #RegExp\n",
    "restore_saver = tf.train.Saver(reuse_vars)  # to restore layers 1-3 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/GradClipp_model/GradientClipping-model\n",
      "Epoch  0\tValid accuracy: 0.97300\n",
      "Epoch  1\tValid accuracy: 0.97680\n",
      "Epoch  2\tValid accuracy: 0.97720\n",
      "Epoch  3\tValid accuracy: 0.97780\n",
      "Epoch  4\tValid accuracy: 0.97820\n",
      "Epoch  5\tValid accuracy: 0.97940\n",
      "Epoch  6\tValid accuracy: 0.97940\n",
      "Epoch  7\tValid accuracy: 0.97920\n",
      "Epoch  8\tValid accuracy: 0.97980\n",
      "Epoch  9\tValid accuracy: 0.97960\n",
      "Epoch 10\tValid accuracy: 0.97960\n",
      "Epoch 11\tValid accuracy: 0.98040\n",
      "Epoch 12\tValid accuracy: 0.97980\n",
      "Epoch 13\tValid accuracy: 0.97920\n",
      "Epoch 14\tValid accuracy: 0.97960\n",
      "Epoch 15\tValid accuracy: 0.98060\n",
      "Epoch 16\tValid accuracy: 0.98020\n",
      "Epoch 17\tValid accuracy: 0.98040\n",
      "Epoch 18\tValid accuracy: 0.98040\n",
      "Epoch 19\tValid accuracy: 0.98020\n",
      "Total training time: 14.22\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"../models/GradClipp_model/GradientClipping-model\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping_NEW\", model=True, subdirname=\"GradClipp_model\"))\n",
    "t1 = time.time()\n",
    "print(\"Total training time: {:.2f}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 20 # new\n",
    "n_outputs = 10 # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")  # freezed\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")  # freezed\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")  #RegExp\n",
    "restore_saver = tf.train.Saver(reuse_vars)  # to restore layers 1-3 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/GradClipp_model/GradientClipping-model\n",
      "Epoch  0\tValid accuracy: 0.97420\n",
      "Epoch  1\tValid accuracy: 0.97600\n",
      "Epoch  2\tValid accuracy: 0.97820\n",
      "Epoch  3\tValid accuracy: 0.97820\n",
      "Epoch  4\tValid accuracy: 0.97880\n",
      "Epoch  5\tValid accuracy: 0.97860\n",
      "Epoch  6\tValid accuracy: 0.98000\n",
      "Epoch  7\tValid accuracy: 0.97920\n",
      "Epoch  8\tValid accuracy: 0.97980\n",
      "Epoch  9\tValid accuracy: 0.97880\n",
      "Epoch 10\tValid accuracy: 0.97940\n",
      "Epoch 11\tValid accuracy: 0.98020\n",
      "Epoch 12\tValid accuracy: 0.98040\n",
      "Epoch 13\tValid accuracy: 0.98020\n",
      "Epoch 14\tValid accuracy: 0.97960\n",
      "Epoch 15\tValid accuracy: 0.98020\n",
      "Epoch 16\tValid accuracy: 0.98040\n",
      "Epoch 17\tValid accuracy: 0.98020\n",
      "Epoch 18\tValid accuracy: 0.98060\n",
      "Epoch 19\tValid accuracy: 0.97920\n",
      "Total training time: 13.01\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"../models/GradClipp_model/GradientClipping-model\")\n",
    "    for epoch in range(n_epochs):\n",
    "        h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "        h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "        for hidden2_batch, y_batch in shuffle_batch(h2_cache, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"GradientClipping_NEW\", model=True, subdirname=\"GradClipp_model\"))\n",
    "t1 = time.time()\n",
    "print(\"Total training time: {:.2f}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{m}\\leftarrow\\beta\\mathbf{m}-\\eta\\nabla_{\\theta}J(\\theta)$\n",
    "2. $\\theta\\leftarrow\\theta+\\mathbf{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\nabla_{\\theta}J(\\theta)=\\text{const}$:\n",
    "\n",
    "$m=\\eta\\dfrac{1}{1 - \\beta}\\text{const}$ - e.g. if $\\beta=0.9$ the speed will be 10 times higher than normal GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, \n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Acceleration Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{m}\\leftarrow\\beta\\mathbf{m}-\\eta\\nabla_{\\theta}J(\\theta + \\beta\\mathbf{m})$\n",
    "2. $\\theta\\leftarrow\\theta+\\mathbf{m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highly not recommended for DNN - can stop too early (before global optimum is reached)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{s}\\leftarrow\\mathbf{s} + \\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$ ($\\otimes$ - elementwise multiplication)\n",
    "2. $\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{\\mathbf{s}+\\varepsilon}$ ($\\oslash$ - elementwise division)\n",
    "\n",
    "$s_i\\leftarrow s_i + (\\partial J(\\theta)\\:/\\:\\partial\\theta_i)^2$\n",
    "\n",
    "$\\theta_i\\leftarrow\\theta_i-\\eta\\partial J(\\theta)\\:/\\:\\partial\\theta_i\\:/\\:\\sqrt{s_i + \\varepsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{s}\\leftarrow\\beta\\mathbf{s} + (1-\\beta)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$\n",
    "2. $\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{\\mathbf{s}+\\varepsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, \n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam (adaptive moment estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{m}\\leftarrow\\beta_1\\mathbf{m}-(1-\\beta_1)\\nabla_{\\theta}J(\\theta)$\n",
    "2. $\\mathbf{s}\\leftarrow\\beta_2\\mathbf{s} + (1-\\beta_2)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$\n",
    "3. $\\mathbf{m}\\leftarrow\\dfrac{\\mathbf{m}}{1-{\\beta_1}^t}$\n",
    "4. $\\mathbf{s}\\leftarrow\\dfrac{\\mathbf{s}}{1-{\\beta_2}^t}$\n",
    "5. $\\theta\\leftarrow\\theta-\\eta\\mathbf{m}\\oslash\\sqrt{\\mathbf{s}+\\varepsilon}$\n",
    "\n",
    "Ususally $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learining_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learining_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.91800\n",
      "Epoch  1\tValid accuracy: 0.95080\n",
      "Epoch  2\tValid accuracy: 0.96400\n",
      "Epoch  3\tValid accuracy: 0.96880\n",
      "Epoch  4\tValid accuracy: 0.97240\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"LRSchedule\", model=True, subdirname=\"LearningRateSched_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Regularization\n",
    "## $\\ell_1$ and $\\ell_2$ Regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.79000\n",
      "Epoch  1\tValid accuracy: 0.84300\n",
      "Epoch  2\tValid accuracy: 0.86440\n",
      "Epoch  3\tValid accuracy: 0.87800\n",
      "Epoch  4\tValid accuracy: 0.88480\n",
      "Epoch  5\tValid accuracy: 0.88880\n",
      "Epoch  6\tValid accuracy: 0.89180\n",
      "Epoch  7\tValid accuracy: 0.89560\n",
      "Epoch  8\tValid accuracy: 0.89860\n",
      "Epoch  9\tValid accuracy: 0.90220\n",
      "Epoch 10\tValid accuracy: 0.90400\n",
      "Epoch 11\tValid accuracy: 0.90580\n",
      "Epoch 12\tValid accuracy: 0.90580\n",
      "Epoch 13\tValid accuracy: 0.90720\n",
      "Epoch 14\tValid accuracy: 0.90700\n",
      "Epoch 15\tValid accuracy: 0.90740\n",
      "Epoch 16\tValid accuracy: 0.90860\n",
      "Epoch 17\tValid accuracy: 0.90940\n",
      "Epoch 18\tValid accuracy: 0.90920\n",
      "Epoch 19\tValid accuracy: 0.90940\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"L1_Reg\", model=True, subdirname=\"L1_Reg_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, \n",
    "                         kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"L1_Regul\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.82740\n",
      "Epoch  1\tValid accuracy: 0.87660\n",
      "Epoch  2\tValid accuracy: 0.89520\n",
      "Epoch  3\tValid accuracy: 0.90180\n",
      "Epoch  4\tValid accuracy: 0.90840\n",
      "Epoch  5\tValid accuracy: 0.90960\n",
      "Epoch  6\tValid accuracy: 0.91260\n",
      "Epoch  7\tValid accuracy: 0.91540\n",
      "Epoch  8\tValid accuracy: 0.91780\n",
      "Epoch  9\tValid accuracy: 0.91900\n",
      "Epoch 10\tValid accuracy: 0.92000\n",
      "Epoch 11\tValid accuracy: 0.92240\n",
      "Epoch 12\tValid accuracy: 0.92120\n",
      "Epoch 13\tValid accuracy: 0.92280\n",
      "Epoch 14\tValid accuracy: 0.92240\n",
      "Epoch 15\tValid accuracy: 0.92160\n",
      "Epoch 16\tValid accuracy: 0.92180\n",
      "Epoch 17\tValid accuracy: 0.92280\n",
      "Epoch 18\tValid accuracy: 0.92160\n",
      "Epoch 19\tValid accuracy: 0.92140\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"L1_Reg\", model=True, subdirname=\"L1_Reg_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, rate=dropout_rate, training=training)  # dropout\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, rate=dropout_rate, training=training)  # dropout\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, rate=dropout_rate, training=training)  # dropout\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"Dropout\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\tValid accuracy: 0.92300\n",
      "Epoch  1\tValid accuracy: 0.94420\n",
      "Epoch  2\tValid accuracy: 0.95000\n",
      "Epoch  3\tValid accuracy: 0.96140\n",
      "Epoch  4\tValid accuracy: 0.96500\n",
      "Epoch  5\tValid accuracy: 0.96960\n",
      "Epoch  6\tValid accuracy: 0.97240\n",
      "Epoch  7\tValid accuracy: 0.97340\n",
      "Epoch  8\tValid accuracy: 0.97580\n",
      "Epoch  9\tValid accuracy: 0.97520\n",
      "Epoch 10\tValid accuracy: 0.97680\n",
      "Epoch 11\tValid accuracy: 0.97820\n",
      "Epoch 12\tValid accuracy: 0.97640\n",
      "Epoch 13\tValid accuracy: 0.97860\n",
      "Epoch 14\tValid accuracy: 0.97760\n",
      "Epoch 15\tValid accuracy: 0.98020\n",
      "Epoch 16\tValid accuracy: 0.97980\n",
      "Epoch 17\tValid accuracy: 0.97960\n",
      "Epoch 18\tValid accuracy: 0.98080\n",
      "Epoch 19\tValid accuracy: 0.98060\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {:2}\\tValid accuracy: {:.5f}\".format(epoch, accuracy_val))\n",
    "    save_path = saver.save(sess, log_dir(\"Dropout\", model=True, subdirname=\"Dropout_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
